{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host=\"localhost\",user=\"root\",password=\"\",database=\"douban\",port=3306)\n",
    "file = pd.read_sql(\"select pcomment,pvote,pscore from parasite\",con=conn)    #从数据库中读取评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pcomment    12332\n",
       "pvote       12332\n",
       "pscore      12332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allstar40 rating    3050\n",
       "allstar10 rating    2455\n",
       "allstar20 rating    2379\n",
       "allstar30 rating    2233\n",
       "allstar50 rating    1980\n",
       "allstar              235\n",
       "Name: pscore, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[\"pscore\"].value_counts()       # 用户对电影的打分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = file[file[\"pscore\"]!=\"allstar30 rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = file_1[\"pscore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [*map(lambda x: x.split(\"r\")[1].strip(),a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_score(str):\n",
    "    if str in ['50','40']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =  [*map(clf_score,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5030"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = file_1[\"pcomment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一边是水淹陋室，一边是生日聚会。\\n一边是冰冷坚硬的体育馆，一边是豪华柔软的后花园。\\n钱就是熨斗，把一切都烫平了。\\n他们蹭网、住地下室、为了一份工作不择手段。\\n当你觉得他们已经够惨的时候，电影突然反转：他们还不是最惨的，还有更惨的。\\n你富，就一定有比你更富的；你穷，也一定有比你更穷的。\\n贫富差距就像无数面墙，将人分为了三六九等。\\n这面墙，任凭风吹日晒雨淋也岿然屹立，纹丝不动。\\n朴夫妇看起来傻乎乎的，可他们还是越来越有钱；\\n金基泽一家聪明过人，可还是翻不了身。\\n你以为找个好工作，有份好薪水，就摆脱了贫穷的身份，其实还差了十万八千里。\\n你能洗掉衣服的汗味，却洗不掉已经融入血液的地下室的气味。\\n你爬了十层楼、二十层楼，可能才刚刚到达别人的地下室。\\n他们是虫子，所以他们生活艰难；\\n他们是虫子，所以什么都杀不死他们。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections,jieba      # 进行分词和去停用词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhuxi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.908 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "texts_list = [\" \".join(jieba.cut(x)).split(\" \") for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10099"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfile = open(r\"D:\\NLP\\stop_words\\stop_words.txt\",encoding='utf-8')\n",
    "stopwords_lst = sfile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts_list:\n",
    "    for word in text:\n",
    "        if word in stopwords:\n",
    "            text.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fre = collections.Counter()\n",
    "maxlen=0\n",
    "for i in texts_list:\n",
    "\n",
    "    if len(i)>maxlen:\n",
    "        maxlen = len(i)\n",
    "    for word in i:\n",
    "        word_fre[word] +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen    # 短评最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35148"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_fre)   # 语料库词数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,ytrain,ytest =train_test_split(texts_list,label,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 建模分析\n",
    "## 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuxi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = token.texts_to_sequences(x_train)\n",
    "test_num = token.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128,\n",
       " 989,\n",
       " 647,\n",
       " 81,\n",
       " 1328,\n",
       " 3,\n",
       " 105,\n",
       " 933,\n",
       " 479,\n",
       " 933,\n",
       " 371,\n",
       " 33,\n",
       " 2,\n",
       " 933,\n",
       " 46,\n",
       " 843,\n",
       " 5670,\n",
       " 132,\n",
       " 1897,\n",
       " 67,\n",
       " 4,\n",
       " 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = preprocessing.sequence.pad_sequences(train_num,maxlen=maxlen)\n",
    "xtest = preprocessing.sequence.pad_sequences(test_num,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten,Dense,Embedding, LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 包含embedding层和Dense层的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 128)          3840000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1024032   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,864,065\n",
      "Trainable params: 4,864,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(30000,128,input_length=250))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7069 samples, validate on 3030 samples\n",
      "Epoch 1/20\n",
      "7069/7069 [==============================] - 6s 898us/step - loss: 0.6694 - acc: 0.6325 - val_loss: 0.5204 - val_acc: 0.8036\n",
      "Epoch 2/20\n",
      "7069/7069 [==============================] - 5s 722us/step - loss: 0.3493 - acc: 0.8625 - val_loss: 0.3665 - val_acc: 0.8422\n",
      "Epoch 3/20\n",
      "7069/7069 [==============================] - 5s 719us/step - loss: 0.1740 - acc: 0.9352 - val_loss: 0.3623 - val_acc: 0.8528\n",
      "Epoch 4/20\n",
      "7069/7069 [==============================] - 5s 718us/step - loss: 0.0976 - acc: 0.9670 - val_loss: 0.3660 - val_acc: 0.8564\n",
      "Epoch 5/20\n",
      "7069/7069 [==============================] - 5s 718us/step - loss: 0.0547 - acc: 0.9819 - val_loss: 0.3973 - val_acc: 0.8617\n",
      "Epoch 6/20\n",
      "7069/7069 [==============================] - 5s 742us/step - loss: 0.0300 - acc: 0.9902 - val_loss: 0.4533 - val_acc: 0.8578\n",
      "Epoch 7/20\n",
      "7069/7069 [==============================] - 6s 779us/step - loss: 0.0159 - acc: 0.9966 - val_loss: 0.5265 - val_acc: 0.8521\n",
      "Epoch 8/20\n",
      "7069/7069 [==============================] - 5s 778us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.5741 - val_acc: 0.8479\n",
      "Epoch 9/20\n",
      "7069/7069 [==============================] - 6s 780us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.6875 - val_acc: 0.8446\n",
      "Epoch 10/20\n",
      "7069/7069 [==============================] - 6s 779us/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.7153 - val_acc: 0.8488\n",
      "Epoch 11/20\n",
      "7069/7069 [==============================] - 6s 781us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.7664 - val_acc: 0.8422\n",
      "Epoch 12/20\n",
      "7069/7069 [==============================] - 6s 806us/step - loss: 0.0025 - acc: 0.9990 - val_loss: 0.9692 - val_acc: 0.8320\n",
      "Epoch 13/20\n",
      "7069/7069 [==============================] - 6s 785us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.8404 - val_acc: 0.8370\n",
      "Epoch 14/20\n",
      "7069/7069 [==============================] - 6s 799us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.8875 - val_acc: 0.8356\n",
      "Epoch 15/20\n",
      "7069/7069 [==============================] - 6s 784us/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.9716 - val_acc: 0.8403\n",
      "Epoch 16/20\n",
      "7069/7069 [==============================] - 6s 806us/step - loss: 0.0019 - acc: 0.9992 - val_loss: 0.9701 - val_acc: 0.8347\n",
      "Epoch 17/20\n",
      "7069/7069 [==============================] - 6s 897us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 1.0927 - val_acc: 0.8310\n",
      "Epoch 18/20\n",
      "7069/7069 [==============================] - 6s 866us/step - loss: 0.0018 - acc: 0.9993 - val_loss: 1.0996 - val_acc: 0.8277\n",
      "Epoch 19/20\n",
      "7069/7069 [==============================] - 6s 838us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 1.1295 - val_acc: 0.8300\n",
      "Epoch 20/20\n",
      "7069/7069 [==============================] - 6s 800us/step - loss: 0.0018 - acc: 0.9992 - val_loss: 1.0433 - val_acc: 0.8310\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xtrain,ytrain,epochs=20,batch_size=128,validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 包含emdedding层和LSTM层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 256)          7680000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32)                36992     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,717,025\n",
      "Trainable params: 7,717,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(30000,256,input_length=250))\n",
    "model1.add(LSTM(32,dropout=0.3,recurrent_dropout=0.2))\n",
    "# model1.add(LSTM(32,dropout=0.3,recurrent_dropout=0.2))\n",
    "model1.add(Dense(1,activation='sigmoid',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.001)))\n",
    "model1.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7069 samples, validate on 3030 samples\n",
      "Epoch 1/10\n",
      "7069/7069 [==============================] - 32s 5ms/step - loss: 0.1101 - acc: 0.9919 - val_loss: 0.6077 - val_acc: 0.8564\n",
      "Epoch 2/10\n",
      "7069/7069 [==============================] - 37s 5ms/step - loss: 0.1064 - acc: 0.9939 - val_loss: 0.6346 - val_acc: 0.8502\n",
      "Epoch 3/10\n",
      "7069/7069 [==============================] - 39s 6ms/step - loss: 0.1019 - acc: 0.9942 - val_loss: 0.6645 - val_acc: 0.8488\n",
      "Epoch 4/10\n",
      "7069/7069 [==============================] - 37s 5ms/step - loss: 0.0990 - acc: 0.9952 - val_loss: 0.6396 - val_acc: 0.8479\n",
      "Epoch 5/10\n",
      "7069/7069 [==============================] - 38s 5ms/step - loss: 0.0976 - acc: 0.9949 - val_loss: 0.6483 - val_acc: 0.8459\n",
      "Epoch 6/10\n",
      "7069/7069 [==============================] - 37s 5ms/step - loss: 0.0941 - acc: 0.9962 - val_loss: 0.6315 - val_acc: 0.8515\n",
      "Epoch 7/10\n",
      "7069/7069 [==============================] - 36s 5ms/step - loss: 0.0918 - acc: 0.9967 - val_loss: 0.6415 - val_acc: 0.8485\n",
      "Epoch 8/10\n",
      "7069/7069 [==============================] - 39s 5ms/step - loss: 0.0912 - acc: 0.9975 - val_loss: 0.6425 - val_acc: 0.8462\n",
      "Epoch 9/10\n",
      "7069/7069 [==============================] - 36s 5ms/step - loss: 0.0895 - acc: 0.9969 - val_loss: 0.6879 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "7069/7069 [==============================] - 37s 5ms/step - loss: 0.0887 - acc: 0.9977 - val_loss: 0.6773 - val_acc: 0.8475\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(xtrain,ytrain,epochs=10,batch_size=128,validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1129847e-05],\n",
       "       [9.9969923e-01],\n",
       "       [4.0531158e-06],\n",
       "       ...,\n",
       "       [1.4315336e-02],\n",
       "       [8.9548033e-01],\n",
       "       [7.3639864e-01]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9114033784334342"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(ytest,model.predict(xtest))    # 模型的roc值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(texts_list,size=200,sg=1,min_count=1)\n",
    "\n",
    "a=model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(text,model,num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype='float32')\n",
    "    nwords=0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in text:\n",
    "        if word in index2word_set:\n",
    "            nwords +=1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getTextsVec(texts,model,num_features):\n",
    "    counter = 0\n",
    "    textsVec = np.zeros((len(texts),num_features),dtype='float32')\n",
    "    \n",
    "    for text in texts:\n",
    "        textsVec[counter] = makeFeatureVec(text,model,num_features)\n",
    "        counter +=1\n",
    "    return textsVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuxi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "C:\\Users\\zhuxi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "xtrainVec = getTextsVec(x_train,model,200)\n",
    "xtestVec = getTextsVec(x_test,model,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainVec = pd.DataFrame(xtrainVec)\n",
    "tra_vec = xtrainVec.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7069, 200)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtestVec = pd.DataFrame(xtestVec)\n",
    "tes_vec = xtestVec.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3030, 200)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63993399339934"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(tra_vec,ytrain)\n",
    "clf.score(tes_vec,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78429986, 0.7758133 , 0.76874116, 0.77298444, 0.77140835])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = rf(n_estimators=300)\n",
    "cross_val_score(rfc,tra_vec,ytrain,cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lists = [\" \".join(jieba.cut(x)) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xval,y_train,y_val =train_test_split(text_lists,label,test_size=0.3,random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'部分 黑色幽默 。 \\n 认为 三个 小时 完全 Ok ， 满满 料 。 \\n “ 前进 了 一步 ， 后退 了 两步 。 ” 我 喜欢 他们 的 互相理解 。 \\n 被 世界 抛弃 的 阿姨 生不逢时 。 \\n 好美   ... \\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words=stopwords,max_features=3000, ngram_range=(1,2),lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=3000, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtra=tf.transform(xtrain).toarray()\n",
    "xval = tf.transform(xval).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7940594059405941"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(xtra,y_train)\n",
    "clf.score(xval,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于对电影短评的情感分析，对于短评的前期处理，首先采用jieba分词，然后分别采用了word2vec和tf-idf得到词向量，之后利用sklearn中的朴素贝叶斯和随机森林算法建模，分类结果均较差。对于LSTM和神经网络建模，由于模型中添加了embedding层，因此在预处理时未得到词向量，LSTM的分类结果略优于神经网络，但计算耗时却是普通神经网络的7倍左右，两类模型的auc值均在0.86左右，模型均有待提高，目前评论数据中，正类和负类样品数均为5000左右，样本数据偏少，这是模型过拟合的原因之一。对于朴素贝叶斯算法的结果，需要进一步分析研究。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
